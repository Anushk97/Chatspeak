title,link,snippet
Towards a Deep and Unified Understanding of Deep Neural ...,https://proceedings.mlr.press/v97/guan19a.html,We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing (NLP) ...
Parameter-Efficient Transfer Learning for NLP,https://proceedings.mlr.press/v97/houlsby19a.html,"Abstract. Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is ..."
Blessing of Class Diversity in Pre-training,https://proceedings.mlr.press/v206/zhao23a.html,This paper presents a new statistical analysis aiming to explain the recent superior achievements of the pre-training techniques in natural language processing ...
Describing Differences between Text Distributions with ...,https://proceedings.mlr.press/v162/zhong22a/zhong22a.pdf,"We propose to automatically describe the differences by “learning a natural lan- guage hypothesis”: given two distributions D0 and D1, we search for a ..."
Towards a Deep and Unified Understanding of Deep Neural ...,http://proceedings.mlr.press/v97/guan19a/guan19a.pdf,"By examining these issues, we move towards a deep (aware of intermediate layers) and unified (coherent) understanding of neural models. We use models in NLP as ..."
Parameter-Efficient Transfer Learning for NLP,https://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf,"Transfer from pre-trained models yields strong performance on many NLP tasks (Dai & Le, 2015; Howard & Ruder,. 2018; Radford et al., 2018). BERT, a Transformer ..."
NLP From Scratch Without Large-Scale Pretraining,https://proceedings.mlr.press/v162/yao22c/yao22c.pdf,"Pretrained language models have become the stan- dard approach for many NLP tasks due to strong performance, but they are very expensive to train."
Controlled Text Generation with Natural Language Instructions,https://proceedings.mlr.press/v202/zhou23g.html,We annotate natural texts through a combination of off-the-shelf NLP tools and simple heuristics with the linguistic and extra-linguistic constraints they ...
Scaling Up Visual and Vision-Language Representation ...,https://proceedings.mlr.press/v139/jia21b.html,A simple dual-encoder architecture learns to align visual and language representations of the image and text pairs using a contrastive loss. We show that the ...
TOPOLOGICAL STRUCTURE IN NATURAL LANGUAGE DATA,https://proceedings.mlr.press/v196/fitz22a/fitz22a.pdf,"This paper presents a novel method, based on the ideas from algebraic topology, for the analysis of raw natural language text. The paper introduces the ..."
What Language Model Architecture and Pretraining Objective ...,https://proceedings.mlr.press/v162/wang22u.html,"Abstract. Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that ..."
Sketching Transformed Matrices with Applications to Natural ...,https://proceedings.mlr.press/v108/liang20a.html,"Many machine learning applications indeed need to deal with such large transformed matrices, for example word embedding method in NLP needs to work with the ..."
Retrieval Augmented Language Model Pre-Training,https://proceedings.mlr.press/v119/guu20a.html,"To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model ..."
How could Neural Networks understand Programs?,https://proceedings.mlr.press/v139/peng21b.html,"How could Neural Networks understand Programs?Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, Tie-Yan LiuSemantic understand..."
Calibration of Natural Language Understanding Models with ...,https://proceedings.mlr.press/v179/giovannotti22a/giovannotti22a.pdf,We test their performance over a set of diverse NLU tasks and show that they are capable of producing well-calibrated probabilistic predictions that are ...
Large Language Models Can Be Easily Distracted by ...,https://proceedings.mlr.press/v202/shi23a.html,%0 Conference Paper %T Large Language Models Can Be Easily Distracted by Irrelevant Context %A Freda Shi %A Xinyun Chen %A Kanishka Misra %A Nathan Scales %A ...
Towards Understanding Situated Natural Language,https://proceedings.mlr.press/v9/bordes10a.html,Our method allows both world knowledge and linguistic information to be used during learning and prediction. We show experimentally that we can learn to use ...
A journey into the Generative AI and large language models,https://proceedings.mlr.press/v217/elnaggar23a.html,"Abstract. In the last year, the generative AI field has seen a remarkable breakthrough, specifically the generative ai models and their ..."
The Natural Language of Actions,https://proceedings.mlr.press/v97/tennenholtz19a.html,"The Natural Language of ActionsGuy Tennenholtz, Shie MannorWe introduce Act2Vec, a general framework for learning context-based action representation fo..."
Efficient Training of Language Models using Few-Shot Learning,https://proceedings.mlr.press/v202/j-reddi23a.html,"We show that, by leveraging the fast learning nature of few-shot learners, one can train language models efficiently in a stagewise manner. Our main ..."
