title,link,snippet
Character-LLM: A Trainable Agent for Role-Playing,https://aclanthology.org/2023.emnlp-main.814,"Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide ..."
LLM-Blender: Ensembling Large Language Models with ...,https://aclanthology.org/2023.acl-long.792,"We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source ..."
Large Language Models Can Self-Improve,https://aclanthology.org/2023.emnlp-main.67,"Abstract. Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision."
Can Large Language Models Be an Alternative to Human ...,https://aclanthology.org/2023.acl-long.870,We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result ...
NLP Evaluation in trouble: On the Need to Measure LLM ...,https://aclanthology.org/2023.findings-emnlp.722,Abstract. In this position paper we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble.
The Internal State of an LLM Knows When It's Lying,https://aclanthology.org/2023.findings-emnlp.68.pdf,"While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks."
Towards LLM-based Fact Verification on News Claims with ...,https://aclanthology.org/2023.ijcnlp-main.64.pdf,"We investigate the ability of LLMs with ICL for news claim verification. And we find that with only four-shot demonstration examples, LLMs can outperform most ..."
LLM-Eval: Unified Multi-Dimensional Automatic Evaluation ...,https://aclanthology.org/2023.nlp4convai-1.5,no_snipped
Coupling Large Language Models with Logic ...,https://aclanthology.org/2023.findings-acl.321,"While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models ..."
LLM-Eval: Unified Multi-Dimensional Automatic Evaluation ...,https://aclanthology.org/2023.nlp4convai-1.5,"Abstract. We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language ..."
Text Data Generation with Large Language Models and ...,https://aclanthology.org/2023.acl-long.34,"In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches ..."
LLM Comparative Assessment: Zero-shot NLG Evaluation ...,https://aclanthology.org/2024.eacl-long.8.pdf,Current developments in large language models. (LLMs) have enabled impressive zero-shot ca- pabilities across various natural language tasks ...
LLM Agents in Interaction: Measuring Personality ...,https://aclanthology.org/2024.personalize-1.9,"We condition GPT-3.5 on asymmetric personality profiles to create a population of LLM agents, administer personality tests and submit the agents to a ..."
Is LLM a Reliable Reviewer? A Comprehensive Evaluation ...,https://aclanthology.org/2024.lrec-main.816,"However, can LLM be a qualified and reliable reviewer? Although there already exist several review-related datasets, few works have carefully ..."
LLM-enhanced Self-training for Cross-domain ...,https://aclanthology.org/2023.emnlp-main.508,"To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the ..."
Beat LLMs at Their Own Game: Zero-Shot LLM-Generated ...,https://aclanthology.org/2023.emnlp-main.463,"There are already reported cases of academic cheating by using LLMs. Thus, it is a pressing problem to identify LLM-generated texts. In this work, we design a ..."
GPTCache: An Open-Source Semantic Cache for LLM ...,https://aclanthology.org/2023.nlposs-1.24,"GPTCache2 is an open-source semantic cache that stores LLM responses to address this issue. When integrating an AI application with GPTCache, user queries are ..."
Towards Mitigating LLM Hallucination via Self Reflection,https://aclanthology.org/2023.findings-emnlp.123,"Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this ..."
Leveraging LLM's Evaluation Capabilities for Flexible ...,https://aclanthology.org/2024.findings-eacl.4,"In order to combine the strengths of both paradigms, and overcome their respective shortcomings, we design a new pipeline called “FlexiQA”, in which we utilize ..."
