title,link,snippet
Automated LOINC Standardization Using Pre-trained Large ...,https://proceedings.mlr.press/v193/tu22a/tu22a.pdf,"Our approach, on the other hand, uses embeddings from pre-trained LLM to extract features from text strings, avoid- ing the need for manual feature engineering."
Can Large Language Models Reason about Program ...,https://proceedings.mlr.press/v202/pei23a/pei23a.pdf,"fine-tune an LLM, pre-trained on source code, to take as input the source code of a program, and a target program point, and to output a list of invariants ..."
LLMs Accelerate Annotation for Medical Information Extraction,https://proceedings.mlr.press/v225/goel23a/goel23a.pdf,"Initially, the LLM, conditioned in a few-shot learning setup, generates Base Annota- tions. Subsequently, these annotations are refined by medical annotation ..."
Parallel Diverse Decoding for Large Language Models,https://proceedings.mlr.press/v202/vilnis23a/vilnis23a.pdf,An even lattice of code points parallelizes decoding into diverse high-probability sequences. options for inference are limited. While LLM inference can be ...
Language Models as Zero-Shot Planners: Extracting ...,https://proceedings.mlr.press/v162/huang22a.html,Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The ...
Deja Vu: Contextual Sparsity for Efficient LLMs at Inference ...,https://proceedings.mlr.press/v202/liu23am/liu23am.pdf,"Spar- sity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or."
Do Embodied Agents Dream of Pixelated Sheep,https://proceedings.mlr.press/v202/nottingham23a.html,Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a ...
Reprogramming Pretrained Language Models for Antibody ...,https://proceedings.mlr.press/v202/melnyk23a/melnyk23a.pdf,"Moreover, the sequence-based mod- els typically involve LLM training from scratch on NGS repertoire (Olsen et al., 2022), or GNN training on a small sample ..."
Less is More: Task-aware Layer-wise Distillation for Language ...,https://proceedings.mlr.press/v202/liang23j/liang23j.pdf,"For example, a possible strategy is to use a LLM teacher to generate task-relevant input and output samples in a controllable manner, and then use these samples ..."
Grounding Large Language Models in Interactive ...,https://proceedings.mlr.press/v202/carta23a.html,"In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that ..."
